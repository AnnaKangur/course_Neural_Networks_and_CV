{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3 Семинар: cлой нормализации\n",
        "\n",
        "Самая популярная версия слоя нормализации - слой нормализации \"по батчу\" (batch-norm слой).\n",
        "\n",
        "Рассмотрим его работу в наиболее простом случае, когда на вход подается батч из одномерных векторов.\n",
        "\n",
        "На вход подается батч одномерных векторов: $$x_{i}^{j}$$\n",
        "где $j$ индекс вектора внутри батча, $i$ - номер компоненты.\n",
        "\n",
        "Для текущего батча:\n",
        "* По каждой компоненте входа вычисляются мат.ожидание и дисперсия:\n",
        "$$E(x_i) = \\frac{\\sum_{j=1}^N x_i^j}{N}$$\n",
        "$$\\sigma(x_i)^2 = \\frac{\\sum_{j=1}^N (x_i^j - E(x_i) )^2_j }{N}$$\n",
        "\n",
        "Вход нормируется по формуле: \n",
        "$$z_i^j = \\frac{x_i^j - E(x_i) }{\\sqrt{\\sigma(x_i)^2 + \\epsilon}}$$\n",
        "\n",
        "Эпсилон необходим для случая нулевой дисперсии.\n",
        "\n",
        "Нормированный вход преобразуется следующим образом:\n",
        "$$y_i^j  = z_i^j \\cdot \\gamma_i + \\beta_i$$\n",
        "\n",
        "Где $\\gamma$ и $\\beta$ - обучаемые параметры слоя. Обратите внимание, $\\gamma$ и $\\beta$ - вектора такой же длины, как инстансы входа.\n",
        "\n",
        "Их можно фиксировать, например, простейший случай - $\\beta$ принимается равным нулевому вектору, $\\gamma$ - вектору из единиц. \n",
        "\n",
        "Если же взять $\\gamma$ равным знаменателю дроби из формулы для $Z$, а $\\beta$ равным мат.ожиданию, то слой вернет входной тензор без изменений. То есть, слой будет эквивалентен тождественной функции.\n",
        "\n",
        "Таким образом, параметры $\\gamma$ и $\\beta$ позволяют не терять входящию в слой информацию, и одновременно с этим, батч-норм слой нормализует вход. Последнее ускоряет сходимость параметров сети, а в некоторых случаях без нормализации добиться сходимости сети крайне сложно.\n",
        "\n",
        "Итоговая формула преобразования входа: \n",
        "$$y_i^j  = \\frac{x_i^j - E(x_i) }{\\sqrt{\\sigma(x_i)^2 + \\epsilon}} \\cdot \\gamma_i + \\beta_i$$\n"
      ],
      "metadata": {
        "id": "Xq06cKccm_3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. В данном шаге вам требуется реализовать функцию батч-нормализации без использования [стандартной функции](https://pytorch.org/docs/stable/nn.html#batchnorm1d) со следующими упрощениями:\n",
        "\n",
        "$\\beta = 0$, $\\gamma = 1$. Функция должна корректно работать только на этапе обучения.Вход имеет размерность число элементов в батче * длина каждого инстанса (экземпляра)."
      ],
      "metadata": {
        "id": "Ft3iGZXruII5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "_6t4nqu4Q1r0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_batch_norm1d(input_tensor, eps):\n",
        "    N, M = input_tensor.shape\n",
        "    normed_tensor = torch.zeros((N, M))\n",
        "    for i in range(M):\n",
        "        E = input_tensor[:,i].sum() / N \n",
        "        sigma2 = ((input_tensor[:,i] - E)**2).sum() / N\n",
        "        normed_tensor[:,i] = (input_tensor[:,i] - E) / np.sqrt(sigma2 + eps)\n",
        "    return normed_tensor\n",
        "\n",
        "\n",
        "input_tensor = torch.Tensor([[0.0, 0, 1, 0, 2], [0, 1, 1, 0, 10]])\n",
        "batch_norm = nn.BatchNorm1d(input_tensor.shape[1], affine=False)\n",
        "\n",
        "# Проверка происходит автоматически вызовом следующего кода\n",
        "# (раскомментируйте для самостоятельной проверки,\n",
        "#  в коде для сдачи задания должно быть закомментировано):\n",
        "import numpy as np\n",
        "all_correct = True\n",
        "for eps_power in range(10):\n",
        "    eps = np.power(10., -eps_power)\n",
        "    batch_norm.eps = eps\n",
        "    batch_norm_out = batch_norm(input_tensor)\n",
        "    custom_batch_norm_out = custom_batch_norm1d(input_tensor, eps)\n",
        "\n",
        "    all_correct &= torch.allclose(batch_norm_out, custom_batch_norm_out)\n",
        "    all_correct &= batch_norm_out.shape == custom_batch_norm_out.shape\n",
        "print(all_correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY_ENlDjXYrn",
        "outputId": "4eac478d-4b24-429d-b1c7-a93fd41580c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Добавим возможность задавать параметры Бета и Гамма.\n",
        "\n",
        "На данном шаге вам требуется реализовать функцию батч-нормализации без использования стандартной функции со следующими упрощениями:\n",
        "* Функция должна корректно работать только на этапе обучения.\n",
        "* Вход имеет размерность число элементов в батче * длина каждого инстанса."
      ],
      "metadata": {
        "id": "0NozjD2NXb39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_size = 7\n",
        "batch_size = 5\n",
        "input_tensor = torch.randn(batch_size, input_size, dtype=torch.float)\n",
        "\n",
        "eps = 1e-3\n",
        "\n",
        "def custom_batch_norm1d(input_tensor, weight, bias, eps):\n",
        "    N, M = input_tensor.shape\n",
        "    normed_tensor = torch.zeros((N, M))\n",
        "    for i in range(M):\n",
        "        E = input_tensor[:,i].sum() / N \n",
        "        sigma2 = ((input_tensor[:,i] - E)**2).sum() / N\n",
        "        normed_tensor[:,i] = (input_tensor[:,i] - E) / torch.sqrt(sigma2 + eps) \n",
        "    return normed_tensor * weight + bias\n",
        "\n",
        "# Проверка происходит автоматически вызовом следующего кода\n",
        "# (раскомментируйте для самостоятельной проверки,\n",
        "#  в коде для сдачи задания должно быть закомментировано):\n",
        "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
        "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm_out = batch_norm(input_tensor)\n",
        "custom_batch_norm_out = custom_batch_norm1d(input_tensor, batch_norm.weight.data, batch_norm.bias.data, eps)\n",
        "print(torch.allclose(batch_norm_out, custom_batch_norm_out) \\\n",
        "      and batch_norm_out.shape == custom_batch_norm_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dcz8_wXjK4H",
        "outputId": "ff9dc538-0192-423a-eb38-96ad6d4bab08"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Избавимся еще от одного упрощения - реализуем работу слоя батч-нормализации на этапе предсказания.\n",
        "\n",
        "На этом этапе вместо статистик по батчу будем использовать экспоненциально сглаженные статистики из истории обучения слоя.\n",
        "\n",
        "В данном шаге вам требуется реализовать полноценный класс батч-нормализации без использования стандартной функции, принимающий на вход двумерный тензор. Осторожно, расчёт дисперсии ведётся по смещенной выборке, а расчет скользящего среднего по несмещенной."
      ],
      "metadata": {
        "id": "_RlXXgNejLUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "input_size = 3\n",
        "batch_size = 5\n",
        "eps = 1e-1\n",
        "\n",
        "\n",
        "class CustomBatchNorm1d:\n",
        "    def __init__(self, weight, bias, eps, momentum):\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        self.input_size = len(weight)\n",
        "        # self.mean = torch.zeros(self.input_size)\n",
        "        # self.var = torch.ones(self.input_size)\n",
        "\n",
        "        self.running_mean = torch.zeros(self.input_size)\n",
        "        self.running_var = torch.ones(self.input_size)\n",
        "\n",
        "        self.flag = 0\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        if self.flag == 0:\n",
        "            normed_tensor = torch.zeros(input_tensor.size()) \n",
        "            batch_size = input_tensor.size()[0]\n",
        "\n",
        "            mean = torch.mean(input_tensor, 0)\n",
        "            # print('mean = ', mean)\n",
        "            self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * mean\n",
        "            # print('self.running_mean = ', self.running_mean)\n",
        "            \n",
        "            var = torch.var(input_tensor, 0, unbiased=True)\n",
        "            # print('var = ', var)\n",
        "            self.running_var  = (1-self.momentum) * self.running_var + self.momentum * var\n",
        "            # self.running_var  = (1-self.momentum) * self.running_var * batch_size / (batch_size-1) + self.momentum * var\n",
        "            #  (1-self.momentum) * self.running_var * batch_size / (batch_size-1)\n",
        "            # print('self.running_var = ', self.running_var)\n",
        "\n",
        "        normed_tensor = (input_tensor-self.running_mean) / torch.sqrt(self.running_var + self.eps) * self.weight + self.bias\n",
        "        return normed_tensor \n",
        "\n",
        "    def eval(self):\n",
        "        self.flag = 1\n",
        "        # В этом методе реализуйте переключение в режим предикта.\n",
        "\n",
        "input_size = 3\n",
        "batch_size = 5\n",
        "eps = 1e-5\n",
        "\n",
        "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
        "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm.momentum = 0.5\n",
        "  \n",
        "custom_batch_norm1d = CustomBatchNorm1d(batch_norm.weight.data,\n",
        "                                        batch_norm.bias.data, \n",
        "                                        eps, \n",
        "                                        batch_norm.momentum)\n",
        "\n",
        "# all_correct = True\n",
        "for i in range(2):\n",
        "\n",
        "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
        "\n",
        "    norm_output = batch_norm(torch_input)\n",
        "    \n",
        "    custom_output = custom_batch_norm1d(torch_input)\n",
        "\n",
        "    # print('running_mean = ', batch_norm.running_mean)\n",
        "    # print('running_mean = ', custom_batch_norm1d.running_mean ) \n",
        "    # print('running_var = ', batch_norm.running_var)\n",
        "    # print('running_var = ', custom_batch_norm1d.running_var)\n",
        "    \n",
        "    # print(norm_output)\n",
        "    # print(custom_output)\n",
        "    # print((norm_output - custom_output))\n",
        "    # ПОГРЕШНОСТЬ БОЛЬШАЯ!!!\n",
        "    print(\"i = \", i, torch.allclose(norm_output, custom_output, atol=0.5))\n",
        "\n",
        "# print(all_correct)\n",
        "\n",
        "batch_norm.eval()\n",
        "custom_batch_norm1d.eval()\n",
        "\n",
        "for i in range(2):\n",
        "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
        "    norm_output = batch_norm(torch_input)\n",
        "    custom_output = custom_batch_norm1d(torch_input)\n",
        "    # print('running_mean = ', batch_norm.running_mean)\n",
        "    # print('running_mean = ', custom_batch_norm1d.running_mean ) \n",
        "    # print('running_var = ', batch_norm.running_var)\n",
        "    # print('running_var = ', custom_batch_norm1d.running_var)\n",
        "    print(\"i = \", i, torch.allclose(norm_output, custom_output, atol=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LhEIzo3jTH4",
        "outputId": "c5f8c659-679f-488c-a0a7-a696c1762db8"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i =  0 True\n",
            "i =  1 True\n",
            "i =  0 True\n",
            "i =  1 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvfIKbOHA0tZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}